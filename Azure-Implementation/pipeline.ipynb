{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "Do odpalenia kodu wymagana jest biblioteka **azure-ai-ml**. Jeżeli nie jest zainstalowana należy wywołać komendę *pip install azure-ai-ml*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "gather": {
     "logged": 1687123485810
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name: azure-ai-ml\r\n",
      "Version: 1.5.0\r\n",
      "Summary: Microsoft Azure Machine Learning Client Library for Python\r\n",
      "Home-page: https://github.com/Azure/azure-sdk-for-python\r\n",
      "Author: Microsoft Corporation\r\n",
      "Author-email: azuresdkengsysadmins@microsoft.com\r\n",
      "License: MIT License\r\n",
      "Location: /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages\r\n",
      "Requires: azure-common, azure-core, azure-mgmt-core, azure-storage-blob, azure-storage-file-datalake, azure-storage-file-share, colorama, isodate, jsonschema, marshmallow, msrest, opencensus-ext-azure, pydash, pyjwt, pyyaml, strictyaml, tqdm, typing-extensions\r\n",
      "Required-by: \r\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip show azure-ai-ml"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "### Łączenie się z workspace\n",
    "Kiedy kod wykonywany jest za pomocą wirtualnej maszyny zarządzanej przez Azure ML używamy domyślnych wartości do połączenia się."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "gather": {
     "logged": 1687123489281
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "from azure.identity import DefaultAzureCredential, InteractiveBrowserCredential\n",
    "from azure.ai.ml import MLClient\n",
    "\n",
    "try:\n",
    "    credential = DefaultAzureCredential()\n",
    "    # Check if given credential can get token successfully.\n",
    "    credential.get_token(\"https://management.azure.com/.default\")\n",
    "except Exception as ex:\n",
    "    # Fall back to InteractiveBrowserCredential in case DefaultAzureCredential not work\n",
    "    credential = InteractiveBrowserCredential()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "gather": {
     "logged": 1687123489915
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found the config file in: /config.json\n"
     ]
    }
   ],
   "source": [
    "# Get a handle to workspace\n",
    "ml_client = MLClient.from_config(credential=credential)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "### Tworzenie skryptów\n",
    "Poniższy kod tworzy folder src, w którym znajdą się skrypty pipeline'u. \n",
    "Kolejne 2 bloki kodu zawierają skrypty, które przygotują dane oraz wytrenują model.\n",
    "Przy trenowaniu modelu używam biblioteki mlflow, aby śledzić modele oraz ich artefakty."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "gather": {
     "logged": 1687123489972
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "src folder created\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# create a folder for the script files\n",
    "script_folder = 'src'\n",
    "os.makedirs(script_folder, exist_ok=True)\n",
    "print(script_folder, 'folder created')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting src/prep-data.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile $script_folder/prep-data.py\n",
    "# import libraries\n",
    "import argparse\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "def main(args):\n",
    "    # read data\n",
    "    df = get_data(args.input_data)\n",
    "\n",
    "    cleaned_data = clean_data(df)\n",
    "\n",
    "    scaled_data = scale_data(cleaned_data)\n",
    "\n",
    "    output_df = scaled_data.to_csv((Path(args.output_data) / \"rain.csv\"), index = False)\n",
    "\n",
    "# function that reads the data\n",
    "def get_data(path):\n",
    "    df = pd.read_csv(path)\n",
    "    df.Date = df.Date.astype(np.datetime64)\n",
    "    df.set_index('Date', inplace=True) \n",
    "\n",
    "    # Count the rows and print the result\n",
    "    row_count = (len(df))\n",
    "    print('Preparing {} rows of data'.format(row_count))\n",
    "    \n",
    "    return df\n",
    "\n",
    "# function that removes missing values\n",
    "def clean_data(df):\n",
    "    location_df = df.query(\"Location == 'Darwin'\")  # Wybieram tylko miasto Darwin z całego datasetu, żeby szybciej się liczyło\n",
    "    location_df = location_df.drop(columns='Location')  # Wszędzie jest lokacja darwin, więc wyrzucam tą kolumnę\n",
    "    location_df = location_df.dropna(subset=['RainTomorrow'])  # Wyrzucza wiersze z brakującą wartością zmiennej objaśnianej\n",
    "    location_df = location_df.replace({\"No\": False, \"Yes\": True}).astype(\n",
    "        {'RainToday': bool, 'RainTomorrow': bool})  # Zmienia typ na bool\n",
    "    location_df = location_df.ffill()  # Uzupełniam brakujące dane za pomocą danych z poprzedniego dnia\n",
    "    location_df = pd.get_dummies(\n",
    "        data=location_df)  # One hot encoding - czyli zamieniam dane kategoryczne na zera i jedynki\n",
    "    return location_df\n",
    "\n",
    "def scale_data(df):\n",
    "    # Skalowanie danych - zamienia wartości liczbowe na takie ze średnią w 0 i odchyleniem standardowym 1\n",
    "    scaler = MinMaxScaler()\n",
    "    num_cols = ['MinTemp','MaxTemp','Rainfall','Evaporation','Sunshine','WindGustSpeed','WindSpeed9am', \n",
    "    'WindSpeed3pm', 'Humidity9am', 'Humidity3pm', 'Pressure9am', 'Pressure3pm', 'Cloud9am', 'Cloud3pm',\n",
    "    'Temp9am', 'Temp3pm']\n",
    "    df[num_cols] = scaler.fit_transform(df[num_cols])\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "def parse_args():\n",
    "    # setup arg parser\n",
    "    parser = argparse.ArgumentParser()\n",
    "\n",
    "    # add arguments\n",
    "    parser.add_argument(\"--input_data\", dest='input_data',\n",
    "                        type=str)\n",
    "    parser.add_argument(\"--output_data\", dest='output_data',\n",
    "                        type=str)\n",
    "\n",
    "    # parse args\n",
    "    args = parser.parse_args()\n",
    "\n",
    "    # return args\n",
    "    return args\n",
    "\n",
    "# run script\n",
    "if __name__ == \"__main__\":\n",
    "    # add space in logs\n",
    "    print(\"\\n\\n\")\n",
    "    print(\"*\" * 60)\n",
    "\n",
    "    # parse args\n",
    "    args = parse_args()\n",
    "\n",
    "    # run main function\n",
    "    main(args)\n",
    "\n",
    "    # add space in logs\n",
    "    print(\"*\" * 60)\n",
    "    print(\"\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting src/train-model.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile $script_folder/train-model.py\n",
    "# import libraries\n",
    "import mlflow\n",
    "import glob\n",
    "import argparse\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import roc_curve\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def main(args):\n",
    "    # enable autologging\n",
    "    mlflow.autolog()\n",
    "\n",
    "    # read data\n",
    "    df = get_data(args.training_data)\n",
    "\n",
    "    # split data\n",
    "    X_train, X_test, y_train, y_test = split_data(df)\n",
    "\n",
    "    # train model\n",
    "    model = train_model(args.reg_rate, X_train, X_test, y_train, y_test)\n",
    "\n",
    "    eval_model(model, X_test, y_test)\n",
    "\n",
    "# function that reads the data\n",
    "def get_data(data_path):\n",
    "\n",
    "    all_files = glob.glob(data_path + \"/*.csv\")\n",
    "    df = pd.concat((pd.read_csv(f) for f in all_files), sort=False)\n",
    "    \n",
    "    return df\n",
    "\n",
    "# function that splits the data\n",
    "def split_data(df):\n",
    "    print(\"Splitting data...\")\n",
    "    X = df.drop(columns=\"RainTomorrow\")\n",
    "    y = df.RainTomorrow\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state=101, stratify=y)\n",
    "\n",
    "    return X_train, X_test, y_train, y_test\n",
    "\n",
    "# function that trains the model\n",
    "def train_model(reg_rate, X_train, X_test, y_train, y_test):\n",
    "    mlflow.log_param(\"Regularization rate\", reg_rate)\n",
    "    print(\"Training model...\")\n",
    "    model = LogisticRegression(C=1/reg_rate, solver=\"liblinear\").fit(X_train, y_train)\n",
    "\n",
    "    mlflow.sklearn.save_model(model, args.model_output)\n",
    "\n",
    "    return model\n",
    "\n",
    "# function that evaluates the model\n",
    "def eval_model(model, X_test, y_test):\n",
    "    # calculate accuracy\n",
    "    y_hat = model.predict(X_test)\n",
    "    acc = np.average(y_hat == y_test)\n",
    "    print('Accuracy:', acc)\n",
    "\n",
    "    # calculate AUC\n",
    "    y_scores = model.predict_proba(X_test)\n",
    "    auc = roc_auc_score(y_test,y_scores[:,1])\n",
    "    print('AUC: ' + str(auc))\n",
    "\n",
    "    # plot ROC curve\n",
    "    fpr, tpr, thresholds = roc_curve(y_test, y_scores[:,1])\n",
    "    fig = plt.figure(figsize=(6, 4))\n",
    "    # Plot the diagonal 50% line\n",
    "    plt.plot([0, 1], [0, 1], 'k--')\n",
    "    # Plot the FPR and TPR achieved by our model\n",
    "    plt.plot(fpr, tpr)\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.title('ROC Curve')\n",
    "    plt.savefig(\"ROC-Curve.png\") \n",
    "\n",
    "def parse_args():\n",
    "    # setup arg parser\n",
    "    parser = argparse.ArgumentParser()\n",
    "\n",
    "    # add arguments\n",
    "    parser.add_argument(\"--training_data\", dest='training_data',\n",
    "                        type=str)\n",
    "    parser.add_argument(\"--reg_rate\", dest='reg_rate',\n",
    "                        type=float, default=0.01)\n",
    "    parser.add_argument(\"--model_output\", dest='model_output',\n",
    "                        type=str)\n",
    "\n",
    "    # parse args\n",
    "    args = parser.parse_args()\n",
    "\n",
    "    # return args\n",
    "    return args\n",
    "\n",
    "# run script\n",
    "if __name__ == \"__main__\":\n",
    "    # add space in logs\n",
    "    print(\"\\n\\n\")\n",
    "    print(\"*\" * 60)\n",
    "\n",
    "    # parse args\n",
    "    args = parse_args()\n",
    "\n",
    "    # run main function\n",
    "    main(args)\n",
    "\n",
    "    # add space in logs\n",
    "    print(\"*\" * 60)\n",
    "    print(\"\\n\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "### Definiowanie komponentów\n",
    "Poniższy kod utworzy pliki YAML dla każdego komponentu, który będzie stanowił krok pipeline'u. Do zdefiniowania komponentu należy podać metadane, dane wejściowe, wyjście oraz komendę powłoki, która zostanie użyta do uruchomienia komponentu i jej środowisko wykonawcze."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting prep-data.yml\n"
     ]
    }
   ],
   "source": [
    "%%writefile prep-data.yml\n",
    "$schema: https://azuremlschemas.azureedge.net/latest/commandComponent.schema.json\n",
    "name: prep_data\n",
    "display_name: Prepare training data\n",
    "version: 1\n",
    "type: command\n",
    "inputs:\n",
    "  input_data: \n",
    "    type: uri_file\n",
    "outputs:\n",
    "  output_data:\n",
    "    type: uri_folder\n",
    "code: ./src\n",
    "environment: azureml:AzureML-sklearn-0.24-ubuntu18.04-py37-cpu@latest\n",
    "command: >-\n",
    "  python prep-data.py \n",
    "  --input_data ${{inputs.input_data}}\n",
    "  --output_data ${{outputs.output_data}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting train-model.yml\n"
     ]
    }
   ],
   "source": [
    "%%writefile train-model.yml\n",
    "$schema: https://azuremlschemas.azureedge.net/latest/commandComponent.schema.json\n",
    "name: train_model\n",
    "display_name: Train a decision tree classifier model\n",
    "version: 1\n",
    "type: command\n",
    "inputs:\n",
    "  training_data: \n",
    "    type: uri_folder\n",
    "  reg_rate:\n",
    "    type: number\n",
    "    default: 0.01\n",
    "outputs:\n",
    "  model_output:\n",
    "    type: mlflow_model\n",
    "code: ./src\n",
    "environment: azureml:AzureML-sklearn-0.24-ubuntu18.04-py37-cpu@latest\n",
    "command: >-\n",
    "  python train-model.py \n",
    "  --training_data ${{inputs.training_data}} \n",
    "  --reg_rate ${{inputs.reg_rate}} \n",
    "  --model_output ${{outputs.model_output}} "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "### Ładowanie komponentów"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "gather": {
     "logged": 1686365476421
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "from azure.ai.ml import load_component\n",
    "\n",
    "parent_dir = \"\"\n",
    "\n",
    "prep_data = load_component(source=parent_dir + \"./prep-data.yml\")\n",
    "train_decision_tree = load_component(source=parent_dir + \"./train-model.yml\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "### Ładowanie danych\n",
    "Dane do trenowania modelu mogą znajdować się w trzech formatach:\n",
    "* URI_FILE - jeden plik\n",
    "* URI_FOLDER - folder z plikami\n",
    "* MLTABLE - typ danych specyficzny dla azure, za jego pomocą można odczytać wiele plików w różnych formatach. Jest on wymagany do użycia narzędzi AutoML, jednak kiedy próbowałem tworzyć go w sposób programistyczny to nie chiał działać. Może go jednak również stworzyć za pomocą GUI.  \n",
    "\n",
    "Po wykonaniu poniższego kodu stworzy się data asset. Jeżeli stworzymy kolejny asset o takiej samej nazwie będą one wersjonowane zaczynając od liczby 1, więc do tego możemy się dostać za pomocą \"*azureml:rain-local:1*\", gdybyśmy chcieli uruchomić kolejną wersję tego datasetu to analogicznie będzie to \"*azureml:rain-local:2*\". Testowo stworzyłem kilka wersji danych co zaprezentowane jest na poniższym zdjęciu.\n",
    "<img src=\"images/dataAssets.png\" />"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "gather": {
     "logged": 1686365478885
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Data({'skip_validation': False, 'mltable_schema_url': None, 'referenced_uris': None, 'type': 'uri_file', 'is_anonymous': False, 'auto_increment_version': False, 'name': 'rain-local', 'description': 'Rain in Australia dataset from kaggle', 'tags': {}, 'properties': {}, 'print_as_yaml': True, 'id': '/subscriptions/a09bae00-b244-4265-9803-cfcc15f34f12/resourceGroups/rg-asi/providers/Microsoft.MachineLearningServices/workspaces/mlw-asi/data/rain-local/versions/3', 'Resource__source_path': None, 'base_path': '/mnt/batch/tasks/shared/LS_root/mounts/clusters/ci-asi/code/Users/s20636', 'creation_context': <azure.ai.ml.entities._system_data.SystemData object at 0x7fa49be72f20>, 'serialize': <msrest.serialization.Serializer object at 0x7fa49be73250>, 'version': '3', 'latest_version': None, 'path': 'azureml://subscriptions/a09bae00-b244-4265-9803-cfcc15f34f12/resourcegroups/rg-asi/workspaces/mlw-asi/datastores/workspaceblobstore/paths/LocalUpload/58e3442047b54f8bbbed5b42f21dd66b/weatherAUS.csv', 'datastore': None})"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from azure.ai.ml.entities import Data\n",
    "from azure.ai.ml.constants import AssetTypes\n",
    "\n",
    "my_path = './data/weatherAUS.csv'\n",
    "\n",
    "my_data = Data(\n",
    "    path=my_path,\n",
    "    type=AssetTypes.URI_FILE,\n",
    "    description=\"Rain in Australia dataset from kaggle\",\n",
    "    name=\"rain-local\"\n",
    ")\n",
    "\n",
    "ml_client.data.create_or_update(my_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "### Tworzenie pipeline'u\n",
    "Łączymy oba komponenty. Danymi wejściowymi do modelu jest wcześniej stworzony data asset. Następnie wyjście pierwszego komponentu jest danymi wejściowymi do kolejnego."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "gather": {
     "logged": 1686365479438
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "from azure.ai.ml import Input\n",
    "from azure.ai.ml.constants import AssetTypes\n",
    "from azure.ai.ml.dsl import pipeline\n",
    "\n",
    "@pipeline()\n",
    "def rain_classification(pipeline_job_input):\n",
    "    clean_data = prep_data(input_data=pipeline_job_input)\n",
    "    train_model = train_decision_tree(training_data=clean_data.outputs.output_data)\n",
    "\n",
    "    return {\n",
    "        \"pipeline_job_transformed_data\": clean_data.outputs.output_data,\n",
    "        \"pipeline_job_trained_model\": train_model.outputs.model_output,\n",
    "    }\n",
    "\n",
    "pipeline_job = rain_classification(Input(type=AssetTypes.URI_FILE, path=\"azureml:rain-local:1\"))\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "gather": {
     "logged": 1686365481261
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "display_name: rain_classification\n",
      "type: pipeline\n",
      "inputs:\n",
      "  pipeline_job_input:\n",
      "    type: uri_file\n",
      "    path: azureml:rain-local:1\n",
      "outputs:\n",
      "  pipeline_job_transformed_data:\n",
      "    type: uri_folder\n",
      "  pipeline_job_trained_model:\n",
      "    type: mlflow_model\n",
      "jobs:\n",
      "  clean_data:\n",
      "    type: command\n",
      "    inputs:\n",
      "      input_data:\n",
      "        path: ${{parent.inputs.pipeline_job_input}}\n",
      "    outputs:\n",
      "      output_data: ${{parent.outputs.pipeline_job_transformed_data}}\n",
      "    component:\n",
      "      $schema: https://azuremlschemas.azureedge.net/latest/commandComponent.schema.json\n",
      "      name: prep_data\n",
      "      version: '1'\n",
      "      display_name: Prepare training data\n",
      "      type: command\n",
      "      inputs:\n",
      "        input_data:\n",
      "          type: uri_file\n",
      "      outputs:\n",
      "        output_data:\n",
      "          type: uri_folder\n",
      "      command: python prep-data.py  --input_data ${{inputs.input_data}} --output_data\n",
      "        ${{outputs.output_data}}\n",
      "      environment: azureml:AzureML-sklearn-0.24-ubuntu18.04-py37-cpu@latest\n",
      "      code: /mnt/batch/tasks/shared/LS_root/mounts/clusters/ci-asi/code/Users/s20636/src\n",
      "      is_deterministic: true\n",
      "  train_model:\n",
      "    type: command\n",
      "    inputs:\n",
      "      training_data:\n",
      "        path: ${{parent.jobs.clean_data.outputs.output_data}}\n",
      "    outputs:\n",
      "      model_output: ${{parent.outputs.pipeline_job_trained_model}}\n",
      "    component:\n",
      "      $schema: https://azuremlschemas.azureedge.net/latest/commandComponent.schema.json\n",
      "      name: train_model\n",
      "      version: '1'\n",
      "      display_name: Train a decision tree classifier model\n",
      "      type: command\n",
      "      inputs:\n",
      "        training_data:\n",
      "          type: uri_folder\n",
      "        reg_rate:\n",
      "          type: number\n",
      "          default: '0.01'\n",
      "      outputs:\n",
      "        model_output:\n",
      "          type: mlflow_model\n",
      "      command: 'python train-model.py  --training_data ${{inputs.training_data}}  --reg_rate\n",
      "        ${{inputs.reg_rate}}  --model_output ${{outputs.model_output}} '\n",
      "      environment: azureml:AzureML-sklearn-0.24-ubuntu18.04-py37-cpu@latest\n",
      "      code: /mnt/batch/tasks/shared/LS_root/mounts/clusters/ci-asi/code/Users/s20636/src\n",
      "      is_deterministic: true\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(pipeline_job)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "gather": {
     "logged": 1686365482659
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "# change the output mode\n",
    "pipeline_job.outputs.pipeline_job_transformed_data.mode = \"upload\"\n",
    "pipeline_job.outputs.pipeline_job_trained_model.mode = \"upload\"\n",
    "# set pipeline level compute\n",
    "pipeline_job.settings.default_compute = \"cluster-ASI\"\n",
    "# set pipeline level datastore\n",
    "pipeline_job.settings.default_datastore = \"workspaceblobstore\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "### Uruchamianie pipeline'u "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "gather": {
     "logged": 1686365490010
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[32mUploading src (0.01 MBs):   0%|          | 0/5889 [00:00<?, ?it/s]\r",
      "\u001b[32mUploading src (0.01 MBs): 100%|██████████| 5889/5889 [00:00<00:00, 138656.43it/s]\n",
      "\u001b[39m\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table style=\"width:100%\"><tr><th>Experiment</th><th>Name</th><th>Type</th><th>Status</th><th>Details Page</th></tr><tr><td>pipeline_rain</td><td>red_apricot_579vyxzl52</td><td>pipeline</td><td>Preparing</td><td><a href=\"https://ml.azure.com/runs/red_apricot_579vyxzl52?wsid=/subscriptions/a09bae00-b244-4265-9803-cfcc15f34f12/resourcegroups/rg-asi/workspaces/mlw-asi&amp;tid=ae65f568-0ceb-42c2-9dda-731b9c16e6b4\" target=\"_blank\" rel=\"noopener\">Link to Azure Machine Learning studio</a></td></tr></table>"
      ],
      "text/plain": [
       "PipelineJob({'inputs': {'pipeline_job_input': <azure.ai.ml.entities._job.pipeline._io.base.PipelineInput object at 0x7fa49bdecd30>}, 'outputs': {'pipeline_job_transformed_data': <azure.ai.ml.entities._job.pipeline._io.base.PipelineOutput object at 0x7fa49bded7b0>, 'pipeline_job_trained_model': <azure.ai.ml.entities._job.pipeline._io.base.PipelineOutput object at 0x7fa49bded6c0>}, 'jobs': {}, 'component': PipelineComponent({'auto_increment_version': False, 'source': 'REMOTE.WORKSPACE.JOB', 'is_anonymous': True, 'name': 'azureml_anonymous', 'description': None, 'tags': {}, 'properties': {}, 'print_as_yaml': True, 'id': None, 'Resource__source_path': None, 'base_path': '/mnt/batch/tasks/shared/LS_root/mounts/clusters/ci-asi/code/Users/s20636', 'creation_context': None, 'serialize': <msrest.serialization.Serializer object at 0x7fa49bded720>, 'version': '1', 'latest_version': None, 'schema': None, 'type': 'pipeline', 'display_name': 'rain_classification', 'is_deterministic': None, 'inputs': {'pipeline_job_input': {}}, 'outputs': {'pipeline_job_transformed_data': {}, 'pipeline_job_trained_model': {}}, 'yaml_str': None, 'other_parameter': {}, 'jobs': {'clean_data': Command({'parameters': {}, 'init': False, 'type': 'command', 'status': None, 'log_files': None, 'name': 'clean_data', 'description': None, 'tags': {}, 'properties': {}, 'print_as_yaml': True, 'id': None, 'Resource__source_path': None, 'base_path': '/mnt/batch/tasks/shared/LS_root/mounts/clusters/ci-asi/code/Users/s20636', 'creation_context': None, 'serialize': <msrest.serialization.Serializer object at 0x7fa49bdedc30>, 'allowed_keys': {}, 'key_restriction': False, 'logger': <Logger attr_dict (WARNING)>, 'display_name': None, 'experiment_name': None, 'compute': None, 'services': None, 'comment': None, 'job_inputs': {'input_data': '${{parent.inputs.pipeline_job_input}}'}, 'job_outputs': {'output_data': '${{parent.outputs.pipeline_job_transformed_data}}'}, 'inputs': {'input_data': <azure.ai.ml.entities._job.pipeline._io.base.NodeInput object at 0x7fa49bded360>}, 'outputs': {'output_data': <azure.ai.ml.entities._job.pipeline._io.base.NodeOutput object at 0x7fa49bdece20>}, 'component': 'azureml_anonymous:0c82161d-6b54-43b9-ba81-1941b9172ac6', 'referenced_control_flow_node_instance_id': None, 'kwargs': {'services': None}, 'instance_id': '0ab19bf0-c84a-448c-8973-aa096eb07b67', 'source': 'REMOTE.WORKSPACE.COMPONENT', 'validate_required_input_not_provided': True, 'limits': None, 'identity': None, 'distribution': None, 'environment_variables': {}, 'environment': None, 'resources': None, 'queue_settings': None, 'swept': False}), 'train_model': Command({'parameters': {}, 'init': False, 'type': 'command', 'status': None, 'log_files': None, 'name': 'train_model', 'description': None, 'tags': {}, 'properties': {}, 'print_as_yaml': True, 'id': None, 'Resource__source_path': None, 'base_path': '/mnt/batch/tasks/shared/LS_root/mounts/clusters/ci-asi/code/Users/s20636', 'creation_context': None, 'serialize': <msrest.serialization.Serializer object at 0x7fa49bded3f0>, 'allowed_keys': {}, 'key_restriction': False, 'logger': <Logger attr_dict (WARNING)>, 'display_name': None, 'experiment_name': None, 'compute': None, 'services': None, 'comment': None, 'job_inputs': {'training_data': '${{parent.jobs.clean_data.outputs.output_data}}'}, 'job_outputs': {'model_output': '${{parent.outputs.pipeline_job_trained_model}}'}, 'inputs': {'training_data': <azure.ai.ml.entities._job.pipeline._io.base.NodeInput object at 0x7fa49bded420>}, 'outputs': {'model_output': <azure.ai.ml.entities._job.pipeline._io.base.NodeOutput object at 0x7fa49bded390>}, 'component': 'azureml_anonymous:bb99c5a2-119a-434b-8b1b-2311fcb29e25', 'referenced_control_flow_node_instance_id': None, 'kwargs': {'services': None}, 'instance_id': '3f6736e8-829c-4d7a-a2e6-a0c4006d9fef', 'source': 'REMOTE.WORKSPACE.COMPONENT', 'validate_required_input_not_provided': True, 'limits': None, 'identity': None, 'distribution': None, 'environment_variables': {}, 'environment': None, 'resources': None, 'queue_settings': None, 'swept': False})}, 'job_types': {'command': 2}, 'job_sources': {'REMOTE.WORKSPACE.COMPONENT': 2}, 'source_job_id': None}), 'type': 'pipeline', 'status': 'Preparing', 'log_files': None, 'name': 'red_apricot_579vyxzl52', 'description': None, 'tags': {}, 'properties': {'azureml.DevPlatv2': 'true', 'azureml.DatasetAccessMode': 'Asset', 'azureml.runsource': 'azureml.PipelineRun', 'runSource': 'MFE', 'runType': 'HTTP', 'azureml.parameters': '{}', 'azureml.continue_on_step_failure': 'True', 'azureml.continue_on_failed_optional_input': 'True', 'azureml.enforceRerun': 'False', 'azureml.defaultComputeName': 'cluster-ASI', 'azureml.defaultDataStoreName': 'workspaceblobstore', 'azureml.pipelineComponent': 'pipelinerun'}, 'print_as_yaml': True, 'id': '/subscriptions/a09bae00-b244-4265-9803-cfcc15f34f12/resourceGroups/rg-asi/providers/Microsoft.MachineLearningServices/workspaces/mlw-asi/jobs/red_apricot_579vyxzl52', 'Resource__source_path': None, 'base_path': '/mnt/batch/tasks/shared/LS_root/mounts/clusters/ci-asi/code/Users/s20636', 'creation_context': <azure.ai.ml.entities._system_data.SystemData object at 0x7fa49bded000>, 'serialize': <msrest.serialization.Serializer object at 0x7fa49bdedba0>, 'display_name': 'rain_classification', 'experiment_name': 'pipeline_rain', 'compute': None, 'services': {'Tracking': <azure.ai.ml.entities._job.job_service.JobService object at 0x7fa49bdecfd0>, 'Studio': <azure.ai.ml.entities._job.job_service.JobService object at 0x7fa49bded060>}, 'settings': {}, 'identity': None, 'default_code': None, 'default_environment': None})"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# submit job to workspace\n",
    "pipeline_job = ml_client.jobs.create_or_update(\n",
    "    pipeline_job, experiment_name=\"pipeline_rain\"\n",
    ")\n",
    "pipeline_job"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "Pipeline wykonywał się około 5 minut. Po jego pomyślnym zakończeniu mamy dostęp do wytrenowanego modelu oraz artefaktów.\n",
    "<img src=\"images/pipeline1.png\" />\n",
    "Możemy również zlecić cron job, który będzie uruchamiał pipeline co określony okres czasu.\n",
    "<img src=\"images/pipeline2.png\" />\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AutoML"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aby stworzyć model AutoML w Azure potrzebny jest dataset w formacie MLTable, stworzyłem go za pomocą GUI, ponieważ rozwiązanie używające kodu nie chciało działać i po kilkunastu próbach poddałem się. Poniżej pokazany jest graficzny interfejs do tworzenia data assetów.\n",
    "<img src=\"images/mltable.png\" />\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Po stworzeniu zbioru danych w odpowiednim formacie należy przejść do modułu **Automated ML** po lewej stronie interfejsu. Następnie wybieramy dane oraz konfigurujemy zadanie. W tym przypadku wybieramy klasyfikację oraz podajemy sposób walidacji modeli.\n",
    "<img src=\"images/automl1.png\" />\n",
    "<img src=\"images/automl2.png\" />\n",
    "<img src=\"images/automl3.png\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Eksperyment wykonywał się 3 godziny 9 minut na klastrze złożonym z dwóch jednostek Standard_DS11_v2 (2 cores, 14 GB RAM, 28 GB disk). Łącznie zostało przetestowanych około 50 modeli. Większość z nich trenowała się poniżej 2 minut, ale jeden z nich zakończył się błędem z powodu timeoutu po godzinie.\n",
    "<img src=\"images/automl4.png\" />\n",
    "Po kliknięciu w jeden z modeli możemy zobaczyć jego metryki oraz objaśnienie działania zawierające najważniejsze feature'y.\n",
    "<img src=\"images/automl5.png\" />\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Oczywiście każdy z modeli jest automatycznie zapisywany.Aby model mógł być wersjonowany musimy go zarejestrować.\n",
    "<img src=\"images/register1.png\" />\n",
    "<img src=\"images/register2.png\" />\n",
    "W zakładce **Models** po lewej stronie interfejsu mamy dostęp do wszystkich wersjonowanych modeli. Ich działanie jest podobne do wersjonowania datasetów, które zostało wcześniej opisane.\n",
    "<img src=\"images/models1.png\" />\n",
    "## Deployment\n",
    "Po wybraniu zarejestrowanego modelu można go wdrożyć jako endpoint. \n",
    "<img src=\"images/deploy1.png\" />\n",
    "<img src=\"images/deploy2.png\" />\n",
    "Endpoint uruchamiał się około 20 minut. Po tym czasie można przetestować jego działanie. Aby używać endpointu należy pobrać klucz. Bez niego nie da się autoryzować dostępu.\n",
    "<img src=\"images/endpoint1.png\" />\n",
    "\n",
    "Poniżej znajduje się kod, za pomocą którego można użyć endpointu. Został on wygenerowany przez azure.  Jednak endpoint został wyłączony po jego przetestowaniu (z powodu stałych opłat za każdą godzinę użytkowania) dlatego ten kod nie będzie działał.\n",
    "~~~\n",
    "import urllib.request\n",
    "import json\n",
    "import os\n",
    "import ssl\n",
    "\n",
    "def allowSelfSignedHttps(allowed):\n",
    "    # bypass the server certificate verification on client side\n",
    "    if allowed and not os.environ.get('PYTHONHTTPSVERIFY', '') and getattr(ssl, '_create_unverified_context', None):\n",
    "        ssl._create_default_https_context = ssl._create_unverified_context\n",
    "\n",
    "allowSelfSignedHttps(True) # this line is needed if you use self-signed certificate in your scoring service.\n",
    "\n",
    "# Request data goes here\n",
    "# The example below assumes JSON formatting which may be updated\n",
    "# depending on the format your endpoint expects.\n",
    "# More information can be found here:\n",
    "# https://docs.microsoft.com/azure/machine-learning/how-to-deploy-advanced-entry-script\n",
    "data =  {\n",
    "  \"input_data\": {\n",
    "    \"columns\": [\n",
    "      \"Rainfall\",\n",
    "      \"Evaporation\",\n",
    "      \"Sunshine\",\n",
    "      \"WindGustSpeed\",\n",
    "      \"Humidity3pm\",\n",
    "      \"Pressure3pm\",\n",
    "      \"Cloud3pm\",\n",
    "      \"Temp3pm\",\n",
    "      \"RainToday\"\n",
    "    ],\n",
    "    \"index\": [],\n",
    "    \"data\": []\n",
    "  }\n",
    "}\n",
    "\n",
    "body = str.encode(json.dumps(data))\n",
    "\n",
    "url = 'https://mlw-asi-psrel.northeurope.inference.ml.azure.com/score'\n",
    "# Replace this with the primary/secondary key or AMLToken for the endpoint\n",
    "api_key = ''\n",
    "if not api_key:\n",
    "    raise Exception(\"A key should be provided to invoke the endpoint\")\n",
    "\n",
    "# The azureml-model-deployment header will force the request to go to a specific deployment.\n",
    "# Remove this header to have the request observe the endpoint traffic rules\n",
    "headers = {'Content-Type':'application/json', 'Authorization':('Bearer '+ api_key), 'azureml-model-deployment': 'raininaustraliamodel-1' }\n",
    "\n",
    "req = urllib.request.Request(url, body, headers)\n",
    "\n",
    "try:\n",
    "    response = urllib.request.urlopen(req)\n",
    "\n",
    "    result = response.read()\n",
    "    print(result)\n",
    "except urllib.error.HTTPError as error:\n",
    "    print(\"The request failed with status code: \" + str(error.code))\n",
    "\n",
    "    # Print the headers - they include the requert ID and the timestamp, which are useful for debugging the failure\n",
    "    print(error.info())\n",
    "    print(error.read().decode(\"utf8\", 'ignore'))\n",
    "~~~"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Endpoint możemy też monitorować.\n",
    "<img src=\"images/endpoint2.png\" />\n",
    "Isnieje również opcja automatycznego skalowania w zależności od metryk takich jak na przykład zużycie procesora. \n",
    "<img src=\"images/autoscale1.png\" />"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernel_info": {
   "name": "python310-sdkv2"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "microsoft": {
   "host": {
    "AzureML": {
     "notebookHasBeenCompleted": true
    }
   },
   "ms_spell_check": {
    "ms_spell_check_language": "en"
   }
  },
  "nteract": {
   "version": "nteract-front-end@1.0.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
